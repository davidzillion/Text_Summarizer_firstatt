{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNK2QZ67BMA6iHiXGV5Vc4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidzillion/Text_Summarizer_firstatt/blob/main/TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a557572b-6845-410b-8d41-521ddb7abfae"
      },
      "source": [
        "# **Text Summarizer** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fee08865-9169-4113-8f21-4dd5c69e52a4"
      },
      "source": [
        "**Text summarization is the process of creating a short, coherent, and fluent summary of a longer text document and involves the outlining of the text's major points. ... Two different approaches that are used for text summarization are:**\n",
        "+ Extractive Summarization\n",
        "    + Extractive summarization aims at identifying the salient information that is then extracted and grouped together to form a concise summary.\n",
        "+ Abstractive Summarization\n",
        "    + Abstractive summary generation rewrites the entire document by building internal semantic representation, and then a summary is created using natural language processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V3Mzung1w0v",
        "outputId": "9d91a61c-1018-4ca7-fbab-8bd966292159"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cPWH4TDoGVm"
      },
      "source": [
        "# Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuXVrNRl187y"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 300)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptYw9Z_EoM6s"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW1CQBvc182V"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/TextSummarizer/Reviews.csv', sep=',') # ,nrows=100000\n",
        "reviews = df.copy()\n",
        "reviews\n",
        " \n",
        "# in cleaned_summary[] to avoid error = datatype 'float' has no attribute 'lower' ->\n",
        "reviews = reviews.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQNF9UDQYMaa"
      },
      "source": [
        "# Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLsHF28b5RTT"
      },
      "source": [
        "### Datatypes and shape "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwQYLHSQ18sJ",
        "outputId": "56f8bc42-741f-4b02-e5cc-e1e712091df8"
      },
      "source": [
        "reviews.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568454 entries, 0 to 568453\n",
            "Data columns (total 10 columns):\n",
            " #   Column                  Non-Null Count   Dtype \n",
            "---  ------                  --------------   ----- \n",
            " 0   Id                      568454 non-null  object\n",
            " 1   ProductId               568454 non-null  object\n",
            " 2   UserId                  568454 non-null  object\n",
            " 3   ProfileName             568454 non-null  object\n",
            " 4   HelpfulnessNumerator    568454 non-null  object\n",
            " 5   HelpfulnessDenominator  568454 non-null  object\n",
            " 6   Score                   568454 non-null  object\n",
            " 7   Time                    568454 non-null  object\n",
            " 8   Summary                 568454 non-null  object\n",
            " 9   Text                    568454 non-null  object\n",
            "dtypes: object(10)\n",
            "memory usage: 43.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciZPj1xcYa4t",
        "outputId": "829129c3-586c-4d00-a481-beba1fc03abb"
      },
      "source": [
        "reviews.nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                        568454\n",
              "ProductId                  74258\n",
              "UserId                    256059\n",
              "ProfileName               218417\n",
              "HelpfulnessNumerator         231\n",
              "HelpfulnessDenominator       234\n",
              "Score                          5\n",
              "Time                        3168\n",
              "Summary                   295743\n",
              "Text                      393579\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "F61fuD8ZYLfI",
        "outputId": "5c53d9b2-1743-4f0c-e3fd-dcc49c1a5348"
      },
      "source": [
        "reviews.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "      <td>568454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>568454</td>\n",
              "      <td>74258</td>\n",
              "      <td>256059</td>\n",
              "      <td>218417</td>\n",
              "      <td>231</td>\n",
              "      <td>234</td>\n",
              "      <td>5</td>\n",
              "      <td>3168</td>\n",
              "      <td>295743</td>\n",
              "      <td>393579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>549821</td>\n",
              "      <td>B007JFMH8M</td>\n",
              "      <td>A3OXHLG6DIBRW8</td>\n",
              "      <td>C. F. Hill \"CFH\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350345600</td>\n",
              "      <td>Delicious!</td>\n",
              "      <td>This review will make me sound really stupid, but whatever. I don't really care as long as people find out what's real and can avoid my mistakes.&lt;br /&gt;&lt;br /&gt;I got my wonderful little sweet Bella Bean when she was a few days shy of three years old. She had been bounced around from house to house ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "      <td>913</td>\n",
              "      <td>448</td>\n",
              "      <td>451</td>\n",
              "      <td>303826</td>\n",
              "      <td>270052</td>\n",
              "      <td>363122</td>\n",
              "      <td>1143</td>\n",
              "      <td>2462</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Id  ...                                                                                                                                                                                                                                                                                                         Text\n",
              "count   568454  ...                                                                                                                                                                                                                                                                                                       568454\n",
              "unique  568454  ...                                                                                                                                                                                                                                                                                                       393579\n",
              "top     549821  ...  This review will make me sound really stupid, but whatever. I don't really care as long as people find out what's real and can avoid my mistakes.<br /><br />I got my wonderful little sweet Bella Bean when she was a few days shy of three years old. She had been bounced around from house to house ...\n",
              "freq         1  ...                                                                                                                                                                                                                                                                                                          199\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDrKXtbAY2Wo",
        "outputId": "865a50c9-e50c-4cd1-a40a-af8275308e37"
      },
      "source": [
        "reviews.isna().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                        0\n",
              "ProductId                 0\n",
              "UserId                    0\n",
              "ProfileName               0\n",
              "HelpfulnessNumerator      0\n",
              "HelpfulnessDenominator    0\n",
              "Score                     0\n",
              "Time                      0\n",
              "Summary                   0\n",
              "Text                      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLankqAa5Hx_"
      },
      "source": [
        "### Drop duplicates and Nan values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XALWOIGv18vU"
      },
      "source": [
        "#dropping duplicates\n",
        "reviews.drop_duplicates(subset=['Text'],inplace=True)\n",
        "#dropping na\n",
        "reviews.dropna(axis=0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vfzN53h6uGI"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZu72-NV18pd"
      },
      "source": [
        "# dictionary for expanding the contractions\n",
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvFUeFGN_5UW"
      },
      "source": [
        "# Text cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy9UHEFt-nnN"
      },
      "source": [
        "* Convert everything to lowercase\n",
        "\n",
        "* Remove HTML tags\n",
        "\n",
        "* Contraction mapping\n",
        "\n",
        "* Remove (â€˜s)\n",
        "\n",
        "* Remove any text inside the parenthesis ( )\n",
        "\n",
        "* Eliminate punctuations and special characters\n",
        "\n",
        "* Remove stopwords\n",
        "\n",
        "* Remove short words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVhZ4NxJAAIm",
        "outputId": "5f79ac96-5ebe-453b-907c-167c5b31f11c"
      },
      "source": [
        "reviews['Text'][:2]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
              "1                                                                             Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H_6I_UACyCq",
        "outputId": "e42800e1-ffac-47ad-90ee-042e2dfb92ee"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh68Odm718cV"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def text_cleaner(text, num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:    #removing short word                                             \n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA77mDT0-uNg"
      },
      "source": [
        "cleaned_text = []\n",
        "for t in reviews['Text']:\n",
        "  cleaned_text.append(text_cleaner(t,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF4WjKs8-uI7",
        "outputId": "54eb69ec-4198-4f34-8154-4996b963f55c"
      },
      "source": [
        "# the first 2 preprocessed reviews from 'Text' column\n",
        "cleaned_text[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better',\n",
              " 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPX3ISTN-uE1"
      },
      "source": [
        "cleaned_summary = []\n",
        "for t in reviews['Summary']:\n",
        "  cleaned_summary.append(text_cleaner(t,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U90dmCwT-uBD",
        "outputId": "8b7da3de-a6ce-4fb6-86ce-7196e0ed0cd8"
      },
      "source": [
        "cleaned_summary[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['good quality dog food', 'not as advertised']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhK38j4J-t-g"
      },
      "source": [
        "reviews['cleaned_text'] = cleaned_text\n",
        "# cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdL06CRz-t7B"
      },
      "source": [
        "reviews['cleaned_summary'] = cleaned_summary\n",
        "# cleaned_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaiDdYjeaMTT"
      },
      "source": [
        "# Drop empty rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCOqOMTx-t0O"
      },
      "source": [
        "reviews.replace('', np.nan, inplace=True)\n",
        "reviews.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "tsfh9J5L-twH",
        "outputId": "9106a4d0-df41-41ee-ecbe-a0b71ca4ca1a"
      },
      "source": [
        "# fix the max length of the sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# fill the lists with sentences\n",
        "for i in reviews['cleaned_text']:\n",
        "  text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in reviews['cleaned_summary']:\n",
        "  summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text' : text_word_count, 'summary' : summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFimMasd-tqZ",
        "outputId": "5070c031-1771-44be-82d3-d57224ea7403"
      },
      "source": [
        "# max majority summary length is 8\n",
        "counter= 0\n",
        "for i in reviews['cleaned_summary']:\n",
        "  if(len(i.split())<=8):\n",
        "    counter=counter+1\n",
        "print(counter/len(reviews['cleaned_summary']) * 100,'% of the summaries have length <= 8.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94.58073835214775 % of the summaries have length <= 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YoVzBGS-tk0"
      },
      "source": [
        "# put max length of reviews to 30\n",
        "max_text_len = 30\n",
        "max_summary_len = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfxKhCNp-tfp"
      },
      "source": [
        "# select all summaries and reviews with legth <= max_summary_len and max_text_len\n",
        "cleaned_text = np.array(reviews['cleaned_text'])\n",
        "cleaned_summary = np.array(reviews['cleaned_summary'])\n",
        "\n",
        "short_text = []\n",
        "short_summary = []\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "  if(len(cleaned_summary[i].split())<= max_summary_len and len(cleaned_text[i].split())<= max_text_len):\n",
        "    short_text.append(cleaned_text[i])\n",
        "    short_summary.append(cleaned_summary[i])\n",
        "\n",
        "df = pd.DataFrame({'text' : short_text, 'summary' : short_summary})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSM-VD27y9CB"
      },
      "source": [
        "### Tokenize start and end of summary, tokens should not appear in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBmOuHwmqsMq"
      },
      "source": [
        "# tokenize start and end of summary, tokens should not appear in data\n",
        "df['summary'] = df['summary'].apply(lambda x : 'strtsum' + x + 'finishim') # strtsum / startsummarization , finshhim "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkgBclfsqr68"
      },
      "source": [
        "# preparing (20%)test and (80%)train set \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(df['text']), np.array(df['summary']), test_size = 0.2, random_state = 42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp2AXFUgrXcT"
      },
      "source": [
        "# Define Text Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TY7azyxqrvB"
      },
      "source": [
        "# convert word sequences to integer sequences and build vocabulary with tokenizer for text and summary\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# tokenize reviews on train data\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azi_6Yk_rirQ"
      },
      "source": [
        "### Rare words as threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6rFeKjNqrhs",
        "outputId": "6215d912-2281-4211-8adf-680342068817"
      },
      "source": [
        "# define threshold \n",
        "# word counts < 4 = rare word\n",
        "thresh = 4\n",
        "\n",
        "counter = 0 # gives number of rare words \n",
        "total_count = 0 # size of vocabulary\n",
        "freq = 0\n",
        "total_freq = 0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "  total_count = total_count + 1\n",
        "  total_freq = total_freq + value\n",
        "  if (value < thresh):\n",
        "    counter = counter + 1\n",
        "    freq = freq + value\n",
        "\n",
        "print((counter/total_count)*100, '% of rare words in vocabulary')\n",
        "print('Total Coverage of rare words: ', (freq/total_freq)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67.79578951445781 % of rare words in vocabulary\n",
            "Total Coverage of rare words:  1.5252281948080555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvsgQBxq-tcH"
      },
      "source": [
        "# (total_count - counter) = top most common words in reviews\n",
        "# tokenizer for reviews on train data\n",
        "\n",
        "x_tokenizer = Tokenizer(num_words = total_count - counter)\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "# convert text sequences into integer sequences\n",
        "x_train_seq = x_tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = x_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# padding zero upto max length\n",
        "x_train = pad_sequences(x_train_seq, maxlen = max_text_len, padding='post')\n",
        "x_test = pad_sequences(x_test_seq, maxlen = max_text_len, padding='post')\n",
        "\n",
        "# size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXMhH_-5nJIa",
        "outputId": "e662dcbb-65b2-41b5-8093-48e7f57e00fc"
      },
      "source": [
        "x_voc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16629"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqbKmtybryPp"
      },
      "source": [
        "# Define Summary Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cwxb367nJEo"
      },
      "source": [
        "# tokenizer for reviews on train data\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9HKIenTvV3-"
      },
      "source": [
        "### Rare words as thresholds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN1ii45BnJAK",
        "outputId": "402cf62e-b3d2-4412-c5fa-ce4f2b0d6d45"
      },
      "source": [
        "# rare word = count < 6\n",
        "\n",
        "thresh = 6\n",
        "\n",
        "counter = 0\n",
        "total_count = 0\n",
        "freq = 0 \n",
        "total_freq = 0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "  total_count = total_count + 1\n",
        "  total_freq = total_freq + value\n",
        "  if(value < thresh):\n",
        "    counter = counter + 1\n",
        "    freq = freq + value\n",
        "\n",
        "print((counter / total_count) * 100, '% of rare words in vocabulary')\n",
        "print('Total Coverage of rare words:', (freq / total_freq) * 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79.2825001541022 % of rare words in vocabulary\n",
            "Total Coverage of rare words: 7.745322673400783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v-0oPmanI8L"
      },
      "source": [
        "# define tokenizer with top most common words for summary\n",
        "# tokenizer for reviews on train data\n",
        "\n",
        "y_tokenizer = Tokenizer(num_words = total_count - counter)\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "# convert text seq into integer seq\n",
        "y_train_seq = y_tokenizer.texts_to_sequences(y_train)\n",
        "y_test_seq = y_tokenizer.texts_to_sequences(y_test)\n",
        "\n",
        "# padding zero upto max length\n",
        "y_train = pad_sequences(y_train_seq, maxlen=max_summary_len, padding='post')\n",
        "y_test = pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "# vocabulary size\n",
        "y_voc = y_tokenizer.num_words + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YpbykJnI4V"
      },
      "source": [
        "# word count of start token = length of train data\n",
        "# y_tokenizer.word_counts['happy']\n",
        "\n",
        "#y_tokenizer.word_counts['strtsum'], len(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiiK-WX3nI0V"
      },
      "source": [
        "# drop rows that contains 'strtsum'(start) and 'finishim'(end) tokens\n",
        "\n",
        "ind = []\n",
        "for i in range(len(y_train)):\n",
        "  counter=0\n",
        "  for j in y_train[i]:\n",
        "    if j != 0:\n",
        "      counter = counter + 1\n",
        "      if(counter==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_train = np.delete(y_train, ind, axis=0)\n",
        "x_train = np.delete(x_train, ind, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Z2npGKnIvy"
      },
      "source": [
        "ind = []\n",
        "for i in range(len(y_test)):\n",
        "  counter=0\n",
        "  for j in y_test[i]:\n",
        "    if j != 0:\n",
        "      counter = counter + 1\n",
        "      if(counter==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_test = np.delete(y_test, ind, axis=0)\n",
        "x_test = np.delete(x_test, ind, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQq68-Wk8jVr"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPNNmXoYKUhH"
      },
      "source": [
        "### The code below shows the creation of customer attention layer using tensorflow keras \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jitTi3o-Emcg"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, inputs_shape):\n",
        "        inputs_shape = inputs_shape if isinstance(inputs_shape, list) else [inputs_shape]\n",
        "\n",
        "        if len(inputs_shape) < 1 or len(inputs_shape) > 2:\n",
        "            raise ValueError(\"AttentionLayer expect one or two inputs.\")\n",
        "\n",
        "        # The first (and required) input is the actual input to the layer\n",
        "        input_shape = inputs_shape[0]\n",
        "\n",
        "        # Expected input shape consists of a triplet: (batch, input_length, input_dim)\n",
        "        if len(input_shape) != 3:\n",
        "            raise ValueError(\"Input shape for AttentionLayer should be of 3 dimension.\")\n",
        "\n",
        "        self.input_length = int(input_shape[1])\n",
        "        self.input_dim = int(input_shape[2])\n",
        "        attention_param_shape = (self.input_dim, 1)\n",
        "\n",
        "        self.attention_param = self.add_weight(\n",
        "            name='attention_param',\n",
        "            shape=attention_param_shape,\n",
        "            initializer='uniform',\n",
        "            trainable=True,\n",
        "            dtype=tf.float32)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        inputs = inputs if isinstance(inputs, list) else [inputs]\n",
        "\n",
        "        if len(inputs) < 1 or len(inputs) > 2:\n",
        "            raise ValueError(\"AttentionLayer expect one or two inputs.\")\n",
        "\n",
        "        actual_input = inputs[0]\n",
        "        mask = inputs[1] if len(inputs) > 2 else None\n",
        "        if mask is not None and not (((len(mask.shape) == 3 and mask.shape[2] == 1) or len(mask.shape) == 2)\n",
        "                                     and mask.shape[1] == self.input_length):\n",
        "            raise ValueError(\"`mask` should be of shape (batch, input_length) or (batch, input_length, 1) \"\n",
        "                             \"when calling an AttentionLayer.\")\n",
        "\n",
        "        assert actual_input.shape[-1] == self.attention_param.shape[0]\n",
        "\n",
        "        # (batch, input_length, input_dim) * (input_dim, 1) ==> (batch, input_length, 1)\n",
        "        attention_weights = K.dot(actual_input, self.attention_param)\n",
        "\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                mask = K.expand_dims(mask, axis=2)  # (batch, input_length, 1)\n",
        "            mask = K.log(mask)\n",
        "            attention_weights += mask\n",
        "\n",
        "        attention_weights = K.softmax(attention_weights, axis=1)  # (batch, input_length, 1)\n",
        "        result = K.sum(actual_input * attention_weights, axis=1)  # (batch, input_length)  [multiplication uses broadcast]\n",
        "        return result, attention_weights\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[2]  # (batch, input_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBozOoDLnIsC"
      },
      "source": [
        "from keras import backend as K \n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out]) #decoder_outputs, attn_out\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30WwZcxNnIpA"
      },
      "source": [
        "# sparse categorical cross entropy as a loss function, convert the integer seq to one-hot vector (due to memory issues)\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7oT7Y7IFYQ"
      },
      "source": [
        "### Here i am monitoring the validation loss (val_loss), the 'early stopping' it is used to stop training the nn at the right time by monitoring a user-specified metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9Mb4YEZnImg"
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beLwyjeVnIjm"
      },
      "source": [
        "# train the model on batch size of 128 and validate it on (20%) the holdout set \n",
        "\n",
        "history = model.fit([x_train,y_train[:,:-1]],\n",
        "                    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n",
        "                    epochs=50,\n",
        "                    callbacks=[early_stop],\n",
        "                    batch_size=128,\n",
        "                    validation_data=([x_test,y_test[:,:-1]], y_test.reshape(y_test.shape[0],y_test.shape[1], 1)[:,1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo34QXuTJv3r"
      },
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT-6Op0onIgz"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t0KXB0tLs1O"
      },
      "source": [
        "### here describe the plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpIAGeIBnIeB"
      },
      "source": [
        "# dictionary to convert index to word for target and source vocabulary\n",
        "\n",
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1mOWI0EL-rA"
      },
      "source": [
        "## Inference for encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy4AFfrqnIYJ"
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# to predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iyq_pCjnIMw"
      },
      "source": [
        "# implementation of the inference process\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPfjf5N8nIH7"
      },
      "source": [
        "# convert Integer seq to a word seq for summary and reviews\n",
        "\n",
        "def seq2sum(input_seq):\n",
        "  newString=''\n",
        "  for i in input_seq:\n",
        "    if((i != 0 and i != target_word_index['strtsum']) and\n",
        "       i != target_word_index['finishim']):\n",
        "      newString = newString + reverse_target_word_index[i] + ' '\n",
        "      return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "  newString=''\n",
        "  for i in input_seq:\n",
        "    if(i != 0):\n",
        "      newString = newString + reverse_source_word_index[i] + ' '\n",
        "      return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB2LKx-vUQTL"
      },
      "source": [
        "for i in range(0,100):\n",
        "  print('Review: ', seq2text(x_train[i]))\n",
        "  print('Original summary: ', seq2sum(y_train[i]))\n",
        "  print('Preducted summary: ', decode_sequence(x_train[i].reshape(1, max_text_len)))\n",
        "  print('\\n')\n",
        "\n",
        "\n",
        "# Output ...\n",
        "# Review: gave caffeine shakes heart anxiety attack plus tastes unbelievably bad stick coffee tea soda thanks \n",
        "# Original summary: hour \n",
        "# Predicted summary:  not worth the money"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bG4bPvhUQPR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8hTKK62qkni"
      },
      "source": [
        "# Shorter Summarizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5a8FkksrTwH"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDF8HgisrVCm"
      },
      "source": [
        "# Generate clean sentences\n",
        "def read_article(file_name) :\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "    \n",
        "    for sentence in article :\n",
        "        print(sentence)\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "        sentences.pop()\n",
        "        \n",
        "        return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxcgS5jdrU4A"
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhKeMDFIrUsp"
      },
      "source": [
        "# similarity matrix\n",
        "def build_similarity_matrix(sentences, stop_words) :\n",
        "    # create empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    \n",
        "    for idx1 in range(len(sentences)) :\n",
        "        for idx2 in range(len(sentences)) :\n",
        "            if idx1 == idx2 : # ignore if bothare same sentences\n",
        "                continue\n",
        "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "                \n",
        "                return similarity_amtrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXvYNVPVrUjL"
      },
      "source": [
        "# Generate Summary Method\n",
        "def generate_summary(file_name, top_n=5) :\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "# read and tokenize text\n",
        "    sentences = read_article(file_name) \n",
        "# similarity matrix across sentences\n",
        "    sentence_similarity_matrix = build_similarity_matrix(sentences, stop_words)\n",
        "# rank sentences in matirx\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "# sort rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked sentence order are\", \n",
        "         ranked_sentence)\n",
        "    \n",
        "    for i in range(top_n) :\n",
        "        summarize_test.append(\" \".join(ranked_sentence[i][1]))\n",
        "        \n",
        "# summarize text\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summmarize_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_t-k2qMrUc9"
      },
      "source": [
        "# read text file and use summarizer\n",
        "example_text = open('/content/drive/My Drive/finished/TextShort.txt', 'r').read().replace('\\n', '')\n",
        "\n",
        "generate_summary(example_text, 4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}